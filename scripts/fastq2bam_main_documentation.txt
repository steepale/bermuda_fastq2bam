#===============================================================================
#
#         FILE: /mnt/home/steepale/bermuda/fastq2bam/scripts/fastq2bam_main_documentation.txt
#
#        USAGE: for documentation purposes, scripts inside
#
#  DESCRIPTION: This script serves as a step by step documentation script for preprocessing
#				and mapping raw reads.
#                
# REQUIREMENTS: ---
#        NOTES: ---
#       AUTHOR: Alec Steep
#			PI: Eben Gering
# 	   CONTACT: alec.steep@gmail.com
#				
#  AFFILIATION: Michigan State University (MSU), East Lansing, MI, United States
#				         USDA ARS Avian Disease and Oncology Lab (ADOL), East Lansing, MI, United States
#				         Technical University of Munich (TUM), Weihenstephan, Germany
#      VERSION: 1.0
#      CREATED: 2017.06.03
#     REVISION:  
#===============================================================================

# Permanent PROJECT DIRECTORY (TUM Cluster)
cd /mnt/home/steepale/bermuda/fastq2bam


# Transfer files and rename them appropriately
# Files transferred to ./data/

# File naming convention
# ./data/[NGI-NAME]/02-FASTQ/[Date]_[instrument_id]_[run_number]_[Flowcell]/[NGI-NAME]_[BCL-CONVERSION-ID]_[LANE]_[READ]_[VOLUME].fastq.gz
# Example: ./data/P4806_126/02-FASTQ/170213_ST-E00198_0201_BHGNYFALXX/P4806_126_S12_L002_R1_001.fastq.gz

# NGI-NAME: Internal NGI sample indentifier
# Date: Date of sequencing
# Flowcell: Flowcell identifier
# Unknown-ID1: Unknown
# Unknown-ID2: Unknown
# BCL-CONVERSION-ID: Indentifier set by bcl2fastq tool while demultiplexing
# LANE: Sequencing lane that the file originates from
# READ: Forward(1) or reverse(2) read indentifier
# VOLUME: Volume index when file is large enough to be split into volumes


# Create a config file with fields of sample identifiers (must be run from project main directory)
# create intermediate config file from absolute path strings
# Header
printf "#ngi-name\tdate\tinstrument_id\trun_number\tflowcell\tbcl-conversion-id\tlane\tread\tvolume\n" > ./data/int_config.txt
# Body
find ./data/ -name "*.fastq.gz" | \
cut -d'/' -f3,5,6 | \
awk -F '[/_.]' '{print $1 "_" $2 "\t" $3 "\t" $4 "\t" $5 "\t" $6 "\t" $9 "\t" $10 "\t" $11 "\t" $12}' >> \
./data/int_config.txt

# Finish with python script
python ./scripts/generate_config.py

# ./scripts/generate_config.py
#################################
import sys
import os

# reference files
lanes_file = './data/D.Wright_16_01_lanes_info.txt'
lib_file = './data/D.Wright_16_01_library_info.txt'
sample_file = './data/D.Wright_16_01_sample_info.txt'
int_config_file = './data/int_config.txt'

# outfile
outfile = open('./data/config.txt', 'w')

# Write header for output file
outfile.write("#ngi_name\tdate\tinstrument_id\trun_number\tflowcell\tbcl-conversion-id\tlane\tread\tvolume\tbarcode\n")

# Create dictionary of sample id's to appropriate annotations
sample_anns2barcode = {}

for line in open(lib_file):
	if not line.startswith('NGI'):
		line = line.rstrip()
		col = line.split('\t')
		ngi_name = col[0]
		barcode = col[1]
		status = col[4]
		if status == 'PASSED':
			sample_anns2barcode[ngi_name] = barcode

# Iterate through reference files and sample strings to fill in config file
for line in open(int_config_file):
	if not line.startswith('#'):
		line = line.rstrip()
		col = line.split('\t')
		# Apply variables
		ngi_name = col[0]
		date = col[1]
		instrument_id = col[2]
		run_number = col[3]
		flowcell = col[4]
		bcl_conversion = col[5]
		lane = col[6]
		read = col[7]
		volume = col[8]
		if ngi_name in sample_anns2barcode.keys():
			barcode = sample_anns2barcode[ngi_name]
		else:
			print('WARNING')
			print(ngi_name+'\t'+date+'\t'+instrument_id+'\t'+run_number+'\t'+flowcell+'\t'+bcl_conversion+'\t'+lane+'\t'+read+'\t'+volume)
		# Print appropriate values to outfile
		outfile.write(ngi_name+'\t'+date+'\t'+instrument_id+'\t'+run_number+'\t'+flowcell+'\t'+bcl_conversion+'\t'+lane+'\t'+read+'\t'+volume+'\t'+barcode+'\n')
# Close outfile to protect EOF
outfile.close()
#################################

### Just for development purposes, will be adjusted in revision (DANGER, could erase data if not backed up elsewhere)
# Extract a small subset of reads
# Each read corresponds to 4 lines

# Make appropriate directories in dev folder
find ./data -type d | sed 's/data/dev/g' > dirs.txt
xargs mkdir -p < dirs.txt

# Add shortened files to dev folder
time for file in `find ./data/P*/02-FASTQ/*/ -name "*.fastq.gz"`
do
out_file=`echo $file | sed 's/.fastq.gz/.fastq/' | sed 's/data/dev/g'`
echo $out_file
bgzip -d -c $file | head -n 100 > $out_file
bgzip -f $out_file
done

# Perform read trimming (low qual and adapters), fastqc analysis, mapping, deduping, and realignment.
# Submit all the runs with python submit script:

# Download and Adjust a dbsnp vcf file. Spaces appear in gene names of INFO field and causes gatk to error. Replace spaces with underscore.
#sed '19939932q;d' /home/users/a.steep/databases/dbsnp/snp/organisms/chicken_9031/VCF/all_reformat.vcf

# Info about dbSNP as of April 25, 2017
#Last Updated: Build 147 (Apr 14, 2016)
#RefSNP Count: 21.3 Million
#SubSNP Count: 57.4 Million
#Assembly: Gallus_gallus-5.0
#Data: Search, FTP
#Genome Data Viewer: Gallus_gallus-5.0

# Here is a download of the dbSNP file:
wget -r -np -nH ftp://ftp.ncbi.nih.gov/snp/organisms/chicken_9031/VCF/*
# VCF files in /home/users/a.steep/databases/dbsnp/snp/organisms/chicken_9031/VCF

python ./scripts/adjust_dbsnp_file.py \
./data/dbsnp/snp/organisms/chicken_9031/VCF/all.vcf \
./data/dbsnp/snp/organisms/chicken_9031/VCF/all_reformat.vcf

# ./scripts/adjust_dbsnp_file.py
#################################
import sys
import os
import re

infile = sys.argv[1]

outfile = open(sys.argv[2], 'w')

for line in open(infile):
	if line[0] == '#':
		outfile.write(line)
	if line[0] != '#':
		line = line.rstrip()
		col = line.split('\t')
		ref = col[3]
		alt = col[4]
		info = col[7]
		if re.search('-', ref):
			ref = 'N'
		if re.search('-', alt):
			alt = 'N'
		if re.search(' ', info):
			info = info.replace(' ', '_') 
		outfile.write(col[0]+'\t'+col[1]+'\t'+col[2]+'\t'+ref+'\t'+alt+'\t'+col[5]+'\t'+col[6]+'\t'+info+'\n')
outfile.close()
#################################

# Galgal5 reference file and supporting annotation files
# https://github.com/hongenxu/MDV_proj/blob/master/make_galgal5_reference.pl

# Sort the dbsnp file, for some reason it is not sorted perfectly
module load picardTools/1.113

java -jar $PICARD/SortVcf.jar \
I=./data/dbsnp/snp/organisms/chicken_9031/VCF/all_reformat.vcf \
O=./data/dbsnp/snp/organisms/chicken_9031/VCF/all_sorted.vcf \
SEQUENCE_DICTIONARY=./data/Galgal5/genome.dict
# Remove the index of output file or else GATK will error
rm ./data/dbsnp/snp/organisms/chicken_9031/VCF/all_sorted.vcf.idx

# Create a file containing just the ngi names of the samples
cut -f1 ./data/int_config.txt | grep -v "^#" | sort | uniq > \
./data/all_samples_dnaseq_bermuda_ngi.txt

### Merge all fastq files based on sample and read group (AKA barcodes)
for sample in `cat ./data/all_samples_dnaseq_bermuda_ngi.txt`
do
for read in 'R1' 'R2'
do
cmd=$(find ./data/$sample -name $sample*$read"_001.fastq.gz" | tr '\n' '\t'|  awk '{print "cat " $1 " " $2 " " $3}')
final_cmd=`echo $cmd" > ./data/fastq_merged/"$sample"_"$read"_merged.fastq.gz"`
echo $final_cmd | sh
done
done

# Sumbit all the sequenced samples from summer 2014
for sample in `cat ./data/all_samples_dnaseq_bermuda_ngi.txt | head -n1`
do
sh ./scripts/fastq2bam_wrapper.sh \
$sample
done

# ./scripts/fastq2bam_wrapper.sh
#####################################
#!/usr/bin/bash

cd /mnt/scratch/steepale/birdman/bermuda/fastq2bam

sample=$1

# Load appropriate modules
module load Python/3.3.2
module load FastQC/0.11.3
module load Trimmomatic/0.33

# Submit job to cluster
qsub -v Var=$sample -N "fastq2bam_"$sample ./scripts/fastq2bam.py

#####################################

# Dev runs
module load FastQC/0.11.3
for sample in `cat ./data/all_samples_dnaseq_bermuda_ngi.txt | sed -n 16,21p`
do
python ./scripts/fastq2bam.py $sample
done

# ./scripts/fastq2bam.py
################################################
#!/opt/software/Python/3.3.2--GCC-4.4.5/bin/python
#PBS -l nodes=1:ppn=1,walltime=01:00:00,mem=10gb
#PBS -j oe
#PBS -v arg1=Var
import sys
import os
import re
from shutil import copyfile

# Change to proper directory
os.chdir('/mnt/scratch/steepale/birdman/bermuda/fastq2bam')

# input sample
in_sample = sys.argv[1]

# reference files
config_file = './data/config.txt'
#trim_adaptor_file = '/home/users/a.steep/Apps/Trimmomatic-0.36/adapters/TruSeq2-PE.fa'
#galgal5_ref = '/home/proj/MDW_genomics/galgal5/galgal5.fa'
#galgal5_ref_index = '/home/proj/MDW_genomics/galgal5/galgal5.fa.fai'
#galgal5_ref_dict = galgal5_ref.replace('.fa', '.dict') 
#dbsnp = '/home/users/a.steep/databases/dbsnp/snp/organisms/chicken_9031/VCF/all_sorted.vcf'

# Program files
#sickle = '/home/users/a.steep/Apps/sickle/sickle'
#bwa = '/home/users/a.steep/Apps/bwa/bwa'
#samtools = '/home/users/a.steep/Apps/samtools/samtools'
#picard = '/home/users/a.steep/Apps/picard/build/libs/picard-2.9.2-4-gb4a02aa-SNAPSHOT-all.jar'
#gatk = '/home/users/a.steep/Apps/gatk/GenomeAnalysisTK.jar'

# Iterate through the config file and grab appropriate sample identifiers and place in dictionary data structure
sample2ids = {}
sample2lanes = {}
sample2run = {}
sample2bcl = {}

# Iterate through config file and fill dictionaries
for line in open(config_file):
	if line[0] != '#':
		line = line.rstrip()
		col = line.split('\t')
		sample_id = col[0]
		date = col[1]
		instrument_id = col[2]
		run_number = col[3]
		flowcell = col[4]
		bcl_conversion = col[5]
		volume = col[8]
		barcode = col[9]
		lane = col[6]
		read = col[7]
		# Different dictionaries and id relationships
		if sample_id not in sample2ids.keys():
			sample2ids[sample_id] = [barcode]
		if sample_id not in sample2run.keys():
			sample2run[sample_id] = set()
			sample2run[sample_id].add(date+';'+instrument_id+';'+run_number+';'+flowcell)
		elif sample_id in sample2run.keys():
			sample2run[sample_id].add(date+';'+instrument_id+';'+run_number+';'+flowcell)
		if sample_id not in sample2lanes.keys():
			sample2lanes[sample_id] = set()
			sample2lanes[sample_id].add(lane)
		elif sample_id in sample2lanes.keys():
			sample2lanes[sample_id].add(lane)
		if sample_id not in sample2bcl.keys():
			sample2bcl[sample_id] = set()
			sample2bcl[sample_id].add(bcl_conversion)
		elif sample_id in sample2bcl.keys():
			sample2bcl[sample_id].add(bcl_conversion)

# Convert the sets to lists
sample2lanes[in_sample] = list(sample2lanes[in_sample])
sample2run[in_sample] = list(sample2run[in_sample])
sample2bcl[in_sample] = list(sample2bcl[in_sample])
# Create empty set of bams files for each lane to be filled in for loop
bams_per_lane_set = set()

# Run through the files quickly and generate a dictionary of files per sample and read direction
for run in sample2run[in_sample]:
	for lane in sample2lanes[in_sample]:
		for bcl_conversion in sample2bcl[in_sample]:
			date = run.split(';')[0]
			instrument_id = run.split(';')[1]
			run_number = run.split(';')[2]
			flowcell = run.split(';')[3]
			barcode = sample2ids[in_sample]
			r1_file_src = './data/'+in_sample+'/02-FASTQ/'+date+'_'+instrument_id+'_'+run_number+'_'+flowcell+'/'+in_sample+'_'+bcl_conversion+'_'+lane+'_R1_001.fastq.gz'
			if os.path.exists(r1_file_src)


### Run a fastqc analysis on individual sequencing runs before preprocessing to assess read quality and adaptor contamination
for run in sample2run[in_sample]:
	for lane in sample2lanes[in_sample]:
		for bcl_conversion in sample2bcl[in_sample]:
			date = run.split(';')[0]
			instrument_id = run.split(';')[1]
			run_number = run.split(';')[2]
			flowcell = run.split(';')[3]
			barcode = sample2ids[in_sample]
			# copy files to new folder and rename if they exist
			fastq_dir = './dev/fastq_all_samples/'
			r1_file_src = './data/'+in_sample+'/02-FASTQ/'+date+'_'+instrument_id+'_'+run_number+'_'+flowcell+'/'+in_sample+'_'+bcl_conversion+'_'+lane+'_R1_001.fastq.gz'
			r1_file_dst = fastq_dir+in_sample+'_'+bcl_conversion+'_'+date+'_'+instrument_id+'_'+run_number+'_'+flowcell+'_'+lane+'_R1_001.fastq.gz'
			r2_file_src = './data/'+in_sample+'/02-FASTQ/'+date+'_'+instrument_id+'_'+run_number+'_'+flowcell+'/'+in_sample+'_'+bcl_conversion+'_'+lane+'_R2_001.fastq.gz'
			r2_file_dst = fastq_dir+in_sample+'_'+bcl_conversion+'_'+date+'_'+instrument_id+'_'+run_number+'_'+flowcell+'_'+lane+'_R2_001.fastq.gz'
			if not os.path.exists(fastq_dir):
				os.makedirs(fastq_dir)
			if os.path.exists(r1_file_src) and os.path.exists(r2_file_src):
				copyfile(r1_file_src, r1_file_dst)
				copyfile(r2_file_src, r2_file_dst)
				out_dir = './analysis/fastqc_before_trim/'
				# Check if output directory exists, if not create it
				if not os.path.exists(out_dir):
					os.makedirs(out_dir)
				# Run FastQC command
				fastqc_cmd = 'fastqc -o'+' '+out_dir+' '+r1_file_dst+' '+r2_file_dst
				print('### Run a fastqc analysis before preprocessing to assess read quality and adaptor contamination')
				os.system(fastqc_cmd)

### Trim off adaptor sequences with trimmomatic and low quality reads
r1_file = "./data/fastq_merged/"$sample"_R1_merged.fastq.gz"
r2_file = "./data/fastq_merged/"$sample"_R2_merged.fastq.gz"
trimm_r1_paired_file = './data/trimmomatic/'+in_sample+'_merged_R1_'+'paired_trimmomatic.fastq.gz'
trimm_r2_paired_file = './data/trimmomatic/'+in_sample+'_merged_R2_'+'paired_trimmomatic.fastq.gz'
trimm_r1_unpaired_file = './data/trimmomatic/'+in_sample+'_merged_R1_'+'unpaired_trimmomatic.fastq.gz'
trimm_r2_unpaired_file = './data/trimmomatic/'+in_sample+'_merged_R2_'+'unpaired_trimmomatic.fastq.gz'
out_dir = './data/trimmomatic/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
# Run command
trimmomatic_cmd = 'java -jar'+' '+trimmomatic+' '+'PE -threads 4'+' '+ \
r1_file+' '+r2_file+' '+ \
trimm_r1_paired_file+' '+trimm_r1_unpaired_file+' '+ \
trimm_r2_paired_file+' '+trimm_r2_unpaired_file+' '+ \
'ILLUMINACLIP:'+trim_adaptor_file+':2:30:10 HEADCROP:9'
print('### Trim off adaptor sequences with trimmomatic and low quality reads')
os.system(trimmomatic_cmd)

### Perform a FastQC analysis after adapter trimming
out_dir = './analysis/fastqc_post_trimmomatic/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
# Run command
fastqc_cmd = 'fastqc -o'+' '+out_dir+' '+\
trimm_r1_paired_file+' '+trimm_r2_paired_file+' '+\
trimm_r1_unpaired_file+' '+trimm_r2_unpaired_file
print('### Perform a FastQC analysis after adapter trimming')
os.system(fastqc_cmd)

### Trim off remaining low quality reads with sickle
sickle_r1_paired_file = './data/sickle/'+in_sample+'_merged_R1_'+'paired_sickle.fastq.gz'
sickle_r2_paired_file = './data/sickle/'+in_sample+'_merged_R2_'+'paired_sickle.fastq.gz'
sickle_singles_pe_file = './data/sickle/'+in_sample+'_merged_'+'singles_pe_sickle.fastq.gz'
sickle_singles_se_file = './data/sickle/'+in_sample+'_merged_'+'singles_se_sickle.fastq.gz'
out_dir = './data/sickle/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
# Run command
sickle_pe_cmd = 'sickle pe -f '+trimm_r1_paired_file+' -r '+trimm_r2_paired_file+\
' -t sanger -o '+sickle_r1_paired_file+' -p '+sickle_r2_paired_file+\
' -s '+sickle_singles_pe_file+' -q 20 -l 50 -g'
sickle_se_cmd = 'sickle se -f '+trimm_r1_unpaired_file+\
' -t sanger -o '+sickle_singles_se_file+' -q 30 -l 50 -g'
print('### Trim off remaining low quality reads with sickle')
os.system(sickle_pe_cmd)
os.system(sickle_se_cmd)

### Perform FastQC post read quality trim via sickle
out_dir = './analysis/fastqc_post_sickle/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
# Run command
fastqc_cmd = 'fastqc -o'+' '+out_dir+' '+\
sickle_r1_paired_file+' '+sickle_r2_paired_file+' '+\
sickle_singles_pe_file+' '+sickle_singles_se_file
print('### Perform FastQC post read quality trim via sickle')
os.system(fastqc_cmd)

	### Read mapping and alignment with bwa
	out_dir = './data/bwa/'
	bwa_sam = './data/bwa/'+in_sample+'_'+lane+'_bwa_nrg_yet.sam'
	# Check if output directory exists, if not create it
	if not os.path.exists(out_dir):
		os.makedirs(out_dir)
	# Check if reference is indexed
	if not os.path.exists(galgal5_ref_index):
		idx_cmd = samtools+' faidx '+galgal5_ref
		os.system(idx_cmd)
	# Run command
	# Reminder: May want ot use -M command to make compatible with picard tools
	bwa_cmd = bwa+' mem -t 4 -T 30 '+galgal5_ref+' '+\
	sickle_r1_paired_file+' '+sickle_r2_paired_file+' > '+bwa_sam
	print('### Read mapping and alignment with bwa')
	os.system(bwa_cmd)

	### Remove trimmed reads
	os.remove(trimm_r1_paired_file)
	os.remove(trimm_r2_paired_file)
	os.remove(trimm_r1_unpaired_file)
	os.remove(trimm_r2_unpaired_file)

	###Add ReadGroups using picard
	out_dir = './data/post_alignment/'
	pic_sam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_bwa_readgroups.sam'
	rgid = in_sample+'_'+lane
	rgpl = "ILLUMINA"
	rgpu = barcode+'_'+lane
	rgsm = in_sample
	rglb = sample_type
	# Check if output directory exists, if not create it
	if not os.path.exists(out_dir):
		os.makedirs(out_dir)
	# Run the command
	pic_cmd = 'java -Xmx40g -jar '+picard+' AddOrReplaceReadGroups INPUT='+bwa_sam+' OUTPUT='+pic_sam_by_lane+' RGID='+rgid+' RGPL='+rgpl+' RGPU='+rgpu+' RGSM='+rgsm+' RGLB='+rglb
	print('###Add readgroups using picard')
	os.system(pic_cmd)
	# Remove sam file
	os.remove(bwa_sam)

	### Sort the sam file
	rg_bam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_bwa_readgroups.bam'
	sort_cmd = 'java -Xmx40g -jar '+picard+' SortSam INPUT='+pic_sam_by_lane+' OUTPUT='+rg_bam_by_lane+' SORT_ORDER=coordinate'
	print('### Sort the sam file')
	os.system(sort_cmd)
	# Remove read group sam
	os.remove(pic_sam_by_lane)

	### Mark duplicates within the bam file
	marked_bam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_bwa_marked.bam'
	metrics_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_dedup_by_lane_metrics.list'
	dupes_cmd = 'java -Xmx40g -jar '+picard+' MarkDuplicates INPUT='+rg_bam_by_lane+' OUTPUT='+marked_bam_by_lane+' METRICS_FILE='+metrics_by_lane+' MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000'
	print('### Mark duplicates within the bam file')
	os.system(dupes_cmd)
	# Remove sorted bam
	os.remove(rg_bam_by_lane)

	### Index the marked by lane bam file
	indexed_bam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_bwa_marked.bai'
	# The bam index is not automatically rewritten if the file exists. Check if file exists. If so, delete it and create a new one
	if os.path.exists(indexed_bam_by_lane):
		os.remove(indexed_bam_by_lane)
	index_bam_cmd = 'java -Xmx40g -jar '+picard+' BuildBamIndex INPUT='+marked_bam_by_lane+' OUTPUT='+indexed_bam_by_lane
	print('### Index the marked by lane bam file')
	os.system(index_bam_cmd)

	### Indel realignment
	# A fasta dict file of reference is required. Check if exists, if not, then generate it
	ref_dict_cmd = 'java -jar '+picard+' CreateSequenceDictionary R='+galgal5_ref+' O='+galgal5_ref_dict
	if not os.path.exists(galgal5_ref_dict):
		print('### Generate dictionary of contigs and length for reference')
		os.system(ref_dict_cmd)
	intervals_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_intervals_by_lane.list'
	realigned_bam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_realigned_by_lane.bam'
	realign_target_cmd = 'java -Xmx40g -jar '+gatk+' -T RealignerTargetCreator -R '+galgal5_ref+' -I '+marked_bam_by_lane+' -o '+intervals_by_lane
	realign_cmd = 'java -Xmx40g -jar '+gatk+' -T IndelRealigner -R '+galgal5_ref+' -I '+marked_bam_by_lane+' -targetIntervals '+intervals_by_lane+' -o '+realigned_bam_by_lane
	print('### Create realignment targets')
	os.system(realign_target_cmd)
	print('### Indel realignment')
	os.system(realign_cmd)
	# Remove the marked bam and index
	os.remove(marked_bam_by_lane)
	os.remove(indexed_bam_by_lane)

	# Add all bam files to cohort list to later merge
	if realigned_bam_by_lane not in bams_per_lane_set:
		bams_per_lane_set.add(realigned_bam_by_lane)

# Move to per sample level

# Convert set of bam files per lane into string
bams_per_lane = ' I='.join(map(str, bams_per_lane_set))

### Merge bam files from all lanes
out_dir = './data/merged/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
merged_bam = './data/merged/'+in_sample+'_all_lanes_merged.bam'
merge_bam_cmd = 'java -Xmx40g -jar '+picard+' MergeSamFiles I='+bams_per_lane+' O='+merged_bam
print('### Merge bam files from all lanes')
os.system(merge_bam_cmd)
# Remove the bam files per lane
for lane in sample2lanes[in_sample]:
	barcode = sample2ids[in_sample][0]
	suffix = sample2ids[in_sample][1]
	sample_type = sample2ids[in_sample][3]
	realigned_bam_by_lane = './data/post_alignment/'+in_sample+'_'+lane+'_realigned_by_lane.bam'
	os.remove(realigned_bam_by_lane)

### Index merged bam file
merged_bam_index = './data/merged/'+in_sample+'_all_lanes_merged.bai'
# The bam index is not automatically rewritten if the file exists. Check if file exists. If so, delete it and create a new one
if os.path.exists(merged_bam_index):
	os.remove(merged_bam_index)
merged_bam_index_cmd = 'java -jar -Xmx40g -jar '+picard+' BuildBamIndex INPUT='+merged_bam+' OUTPUT='+merged_bam_index
print('### Index merged bam file')
os.system(merged_bam_index_cmd)

### Mark duplicates on merged bam file
marked_merged_bam = './data/merged/'+in_sample+'_merged_marked.bam'
metrics_merged = './data/merged/'+in_sample+'_dedup_merged_metrics.txt'
merged_dupes_cmd = 'java -Xmx40g -jar '+picard+' MarkDuplicates INPUT='+merged_bam+' OUTPUT='+marked_merged_bam+' METRICS_FILE='+metrics_merged+' MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000'
print('### Mark duplicates on merged bam file')
os.system(merged_dupes_cmd)
# Remove merged bam file and index
os.remove(merged_bam)
os.remove(merged_bam_index)

### Index merged marked bam file
marked_merged_bam_index = './data/merged/'+in_sample+'_merged_marked.bai'
# The bam index is not automatically rewritten if the file exists. Check if file exists. If so, delete it and create a new one
if os.path.exists(marked_merged_bam_index):
	os.remove(marked_merged_bam_index)
merged_dupes_index_cmd = 'java -Xmx40g -jar '+picard+' BuildBamIndex INPUT='+marked_merged_bam+' OUTPUT='+marked_merged_bam_index
print('### Index merged marked bam file')
os.system(merged_dupes_index_cmd)

### Indel realignment merged bam
intervals_merged = './data/merged/'+in_sample+'_intervals_merged.list'
dedupped_realigned_merged_bam = './data/merged/'+in_sample+'_bwa_rg_dedupped_realigned.bam'
realign_target_merged_cmd = 'java -Xmx40g -jar '+gatk+' -T RealignerTargetCreator -R '+galgal5_ref+' -I '+marked_merged_bam+' -o '+intervals_merged
realign_merged_cmd = 'java -Xmx40g -jar '+gatk+' -T IndelRealigner -R '+galgal5_ref+' -I '+marked_merged_bam+' -targetIntervals '+intervals_merged+' -o '+dedupped_realigned_merged_bam
print('### Create realignment targets merged bam')
os.system(realign_target_merged_cmd)
print('### Indel realignment merged bam')
os.system(realign_merged_cmd)
# Remove the merged marked bam and index
os.remove(marked_merged_bam)
os.remove(marked_merged_bam_index)

### Index the nearly final bam file
dedupped_realigned_merged_bam_index = './data/merged/'+in_sample+'_bwa_rg_dedupped_realigned.bai'
# The bam index is not automatically rewritten if the file exists. Check if file exists. If so, delete it and create a new one
if os.path.exists(dedupped_realigned_merged_bam_index):
	os.remove(dedupped_realigned_merged_bam_index)
final_bam_index_cmd = 'java -Xmx40g -jar '+picard+' BuildBamIndex  INPUT='+dedupped_realigned_merged_bam+' OUTPUT='+dedupped_realigned_merged_bam_index
print('### Index the nearly final bam file')
os.system(final_bam_index_cmd)

### Perform base quality recalibration with gatk
## Analyze patterns of covariation in the sequence dataset
recal_table = './data/merged/'+in_sample+'_recalibration_data.table'
bqsr_1_cmd = 'java -Xmx40g -jar '+gatk+' -T BaseRecalibrator -R '+galgal5_ref+ \
' -I '+dedupped_realigned_merged_bam+' -knownSites '+dbsnp+' -o '+recal_table
print('## Analyze patterns of covariation in the sequence dataset')
os.system(bqsr_1_cmd)

## Perform a second pass to analyze covariation remaining after recalibration
post_recal_table = './data/merged/'+in_sample+'_post_recalibration_data.table'
bqsr_2_cmd = 'java -Xmx40g -jar '+gatk+' -T BaseRecalibrator -R '+galgal5_ref+ \
' -I '+dedupped_realigned_merged_bam+' -knownSites '+dbsnp+' -BQSR '+recal_table+ \
' -o '+post_recal_table
print('## Perform a second pass to analyze covariation remaining after recalibration')
os.system(bqsr_2_cmd)

## Generate before/after plots
out_dir = './analysis/bsqr/'
# Check if output directory exists, if not create it
if not os.path.exists(out_dir):
	os.makedirs(out_dir)
recal_plots = './analysis/bsqr/'+in_sample+'_recalibration_plots.pdf'
bqsr_3_cmd = 'java -Xmx40g -jar '+gatk+' -T AnalyzeCovariates -R '+galgal5_ref+ \
' -before '+recal_table+' -after '+post_recal_table+' -plots '+recal_plots
print('## Generate before/after plots')
os.system(bqsr_3_cmd)

# If an error is expressed the gsalib may need to be installed 
# see http://gatkforums.broadinstitute.org/gatk/discussion/1244 to install package locally in R

## Apply the recalibration to your sequence data
bqsr_merged_bam = './data/merged/'+in_sample+'_bwa_rg_dedupped_realigned_bqsr.bam'
bqsr_4_cmd = 'java -Xmx40g -jar '+gatk+' -T PrintReads -R '+galgal5_ref+ \
' -I '+dedupped_realigned_merged_bam+' -BQSR '+recal_table+' -o '+bqsr_merged_bam
print('## Apply the recalibration to your sequence data')
os.system(bqsr_4_cmd)

### Index the final bam file
bqsr_merged_bam_index = './data/merged/'+in_sample+'_bwa_rg_dedupped_realigned_bqsr.bai'
if os.path.exists(bqsr_merged_bam_index):
	os.remove(bqsr_merged_bam_index)
# Run command
final_bam_index_cmd = 'java -Xmx40g -jar '+picard+' BuildBamIndex  INPUT='+bqsr_merged_bam+' OUTPUT='+bqsr_merged_bam_index
print('### Index the final bam file')
os.system(final_bam_index_cmd)
# Remove the dedupped realigned merged bam and index
os.remove(dedupped_realigned_merged_bam)
os.remove(dedupped_realigned_merged_bam_index)

### Finished Script
print('Fin')
############################
